title: log nginx request to rabbitmq --advance
date: 2014-10-14 17:29:32
tags: nginx
description: "另外的方式提高发送到rabbitmq的性能"
toc: true
category: nginx
comment: true
---
本文基于上篇[博客][1], 提供另外一种方法, 提高nginx发送message发送到rabbitmq的性能.

<!-- more -->

###前言
上篇博客的代码流程是很简单的

> 1. nginx接受请求
> 2. 组装数据
> 3. 建立RabbitMQ的连接, 将组装好的数据发送到RabbitMQ中。

可以看出, 这种方式是很低级的, 每一个请求都消耗一个connection, 就算我们设置了connection pool, 但是测试机,使用ab做测试

	ab -n 10000 -c 300 -p postdata ${url}

当并发超过300的时候, 连接rabbitmq就会频繁失败, 所以我们需要另外一种思路实现我们的目标.

##Openresty - Timer & Share Dict
在openresty的lua module中, 提供了两个特性

> Timer - 提供了类似调度器的功能
> Share Dict - 一种可以在Worker进程共享的Cache.

关于Timer和Share Dict的更多介绍, 可以访问官方文档.

##基本思路
> 1. 将接受的请求组装成数据
> 2. 将数据保存到Share Dict中
> 3. 后台运行timer, 定期检查， 是否flush到rabbitmq中.

##实现过程
我们需要将Share Dict做一个简单的封装, 变成一个Queue的模型, 供Timer去调用，假定timer将消息Batch发送到RabbitMq称之为flush阶段, 简单介绍一下方法.
先介绍两个variable
	
	write_point - 表示累计接收的request count。
	read_point - 表示timer累计从cache中读取的request count。

整个流程分为两个阶段

> 接收请求阶段: 接收一个请求时，将write_point原子性自增1，将当前的write_point作为存放当前request data的key.
> Timer运行阶段:Timer启动后, 读取write_point和read_point, 如果发现read_point<write_point, 开始flush工作, 将read_point自增, 读取read_point作为key的data, 直到read_point=write_point, 此次flush工作结束.

##注意事项和建议
> 使用Openresty的Lock, 保证当前只有一个timer在进行flush工作.
> 对Timer的调度逻辑进行适量优化, 例如可以增加一个变量存放当前cache中request的容量, 当一次flush结束后, 尝试检查该容量值, 一旦容量超出radio, timer可以立即启动, 否则定时启动.
> 增加一个monitor的url， 管理人员可以通过url获取关键Metric, 获取当前系统的健康状况.

  [1]: http://hy2014.github.io/2014/09/24/log-nginx-request-to-rabbitmq/